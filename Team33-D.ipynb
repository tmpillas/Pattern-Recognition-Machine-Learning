{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**ΟΜΑΔΑ 33**\n",
        "\n",
        "*   Κιζιρίδης Δημήτριος ΑΕΜ: 10539\n",
        "*   Μπίλλας Θωμάς Αχιλλέας ΑΕΜ: 10366\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yb9ZynHgZlUv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKGHHJo8scZU"
      },
      "source": [
        "Φόρτωση των δεδομένων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEtR7y46scZZ",
        "outputId": "dccf6519-fa83-4689-af50-bf596e1baebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (8743, 225)\n",
            "Test data shape: (6955, 224)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 - Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, silhouette_score, davies_bouldin_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Φόρτωση των δεδομένων από τα αρχεία CSV\n",
        "train_data = pd.read_csv('datasetTV.csv', header=None)\n",
        "test_data = pd.read_csv('datasetTest.csv', header=None)\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWR5pv4TscZb"
      },
      "source": [
        "Προεπεξεργασία των δεδομένων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S3lBepIascZb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Διαχωρισμός των χαρακτηριστικών και των ετικετών στο training set\n",
        "x = train_data.iloc[:, :-1].values\n",
        "y = train_data.iloc[:, -1].values\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_test = test_data.values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD7-yaQ7scZc"
      },
      "source": [
        "Καλύτερο Μοντέλο"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEFOc8_escZc",
        "outputId": "ef995007-6c6f-418e-9fad-593e5ec4461c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Validation Accuracy for SVM: 0.873642081189251\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.97      0.96       377\n",
            "           2       0.74      0.76      0.75       332\n",
            "           3       0.93      0.94      0.93       349\n",
            "           4       0.93      0.90      0.92       325\n",
            "           5       0.81      0.78      0.80       366\n",
            "\n",
            "    accuracy                           0.87      1749\n",
            "   macro avg       0.87      0.87      0.87      1749\n",
            "weighted avg       0.87      0.87      0.87      1749\n",
            "\n",
            "Best parameters for SVM: {'C': [9], 'gamma': [0.025], 'kernel': ['rbf']}\n",
            "Test data size: 6955\n",
            "Predictions size: 6955\n",
            "Unique labels in predictions: [1 2 3 4 5]\n",
            "\n",
            "Saved predictions size: 6955\n",
            "First 10 predictions: [1 4 5 2 2 4 5 5 1 3]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "#best model = svm\n",
        "#best params = {'C': 9, 'gamma': 0.025, 'kernel': 'rbf'}\n",
        "\n",
        "def fine_tune_SVM(model, param_grid, X_train, y_train, n_jobs=-1, cv=10):\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=n_jobs, cv=cv)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "    return best_model, best_params\n",
        "\n",
        "# Χρήση του βέλτιστου SVM μοντέλου για πρόβλεψη\n",
        "best_params = {'C': [9], 'gamma': [0.025], 'kernel': ['rbf']}\n",
        "SVM = SVC(random_state=42)\n",
        "best_svm, bet_params_svm = fine_tune_SVM(SVM, best_params, X_train, y_train)\n",
        "\n",
        "y_best = best_svm.predict(X_val)\n",
        "accuracy_best_svm = accuracy_score(y_val, y_best)\n",
        "print(f\"Best Validation Accuracy for SVM: {accuracy_best_svm}\")\n",
        "print(classification_report(y_val, y_best))\n",
        "print(f\"Best parameters for SVM: {best_params}\")\n",
        "\n",
        "labels_33 = best_svm.predict(X_test)\n",
        "print(f\"Test data size: {test_data.shape[0]}\")\n",
        "print(f\"Predictions size: {labels_33.shape[0]}\")\n",
        "print(f\"Unique labels in predictions: {np.unique(labels_33)}\")\n",
        "\n",
        "# Αποθήκευση των προβλέψεων σε ένα numpy αρχείο\n",
        "np.save('labels33.npy', labels_33)\n",
        "\n",
        "# Φόρτωση και επαλήθευση\n",
        "saved_predictions = np.load('labels33.npy')\n",
        "print(f\"\\nSaved predictions size: {saved_predictions.shape[0]}\")\n",
        "print(f\"First 10 predictions: {saved_predictions[:10]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================================================================\n"
      ],
      "metadata": {
        "id": "1MA2h358-FBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Συνάρτηση για έλεγχο scaling ή normalization"
      ],
      "metadata": {
        "id": "31pingnc-SPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def check_scaling_or_normalization(data):\n",
        "    \"\"\"\n",
        "    Determines if scaling or normalization is required for a dataset.\n",
        "    Output:\n",
        "    - Recommendation: Scaling, Normalization, or None.\n",
        "    \"\"\"\n",
        "\n",
        "    # Display dataset statistics\n",
        "    print(\"Dataset Loaded Successfully!\")\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    print(data.describe())\n",
        "\n",
        "    # Drop non-numeric columns\n",
        "    numeric_data = data.select_dtypes(include=[np.number])\n",
        "    if numeric_data.shape[1] == 0:\n",
        "        print(\"No numeric features found. Scaling/Normalization is not applicable.\")\n",
        "        return\n",
        "\n",
        "    # Check for scaling/normalization needs\n",
        "    print(\"\\nAnalyzing Feature Ranges...\")\n",
        "\n",
        "    feature_ranges = numeric_data.max() - numeric_data.min()\n",
        "    feature_std = numeric_data.std()\n",
        "\n",
        "    range_ratio = feature_ranges.max() / feature_ranges.min()\n",
        "    std_ratio = feature_std.max() / feature_std.min()\n",
        "\n",
        "    print(f\"Range Ratio (Max/Min): {range_ratio:.2f}\")\n",
        "    print(f\"Standard Deviation Ratio (Max/Min): {std_ratio:.2f}\")\n",
        "\n",
        "    # Recommendations\n",
        "    if range_ratio > 10:\n",
        "        print(\"\\nRecommendation: Normalization is recommended (MinMaxScaler).\")\n",
        "        print(\"Reason: Large differences in feature ranges detected.\")\n",
        "    elif std_ratio > 10:\n",
        "        print(\"\\nRecommendation: Scaling is recommended (StandardScaler).\")\n",
        "        print(\"Reason: Large differences in feature variances detected.\")\n",
        "    else:\n",
        "        print(\"\\nRecommendation: No scaling or normalization required.\")\n",
        "        print(\"Reason: Features are already on a similar scale.\")\n",
        "\n",
        "# Example Usage\n",
        "print(\"Training data:\")\n",
        "check_scaling_or_normalization(train_data)\n",
        "print(\"Testing data:\")\n",
        "check_scaling_or_normalization(test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdPnCHnZ-Ekx",
        "outputId": "47e1a7f9-d336-43dc-a30f-dae746696203"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data:\n",
            "Dataset Loaded Successfully!\n",
            "\n",
            "Basic Statistics:\n",
            "               0            1            2            3            4    \\\n",
            "count  8743.000000  8743.000000  8743.000000  8743.000000  8743.000000   \n",
            "mean      0.360086     0.001636     0.343860     0.177411     0.199805   \n",
            "std       0.545784     0.321264     0.537970     0.479141     0.469726   \n",
            "min      -1.052900    -1.492000    -1.168500    -1.154900    -1.238800   \n",
            "25%      -0.056065    -0.212760    -0.059468    -0.154845    -0.121140   \n",
            "50%       0.273600     0.004490     0.254900     0.102000     0.126890   \n",
            "75%       0.772390     0.212595     0.739950     0.428225     0.449460   \n",
            "max       2.035000     1.582200     2.075400     1.928000     1.934200   \n",
            "\n",
            "               5            6            7            8            9    ...  \\\n",
            "count  8743.000000  8743.000000  8743.000000  8743.000000  8743.000000  ...   \n",
            "mean      0.522868     0.374971     0.515616     0.041620    -0.002033  ...   \n",
            "std       0.559579     0.552721     0.559624     0.359096     0.315393  ...   \n",
            "min      -1.170100    -1.107700    -1.100400    -1.099600    -1.250200  ...   \n",
            "25%       0.075904    -0.055405     0.067974    -0.199340    -0.215180  ...   \n",
            "50%       0.520860     0.289280     0.502110     0.022291    -0.003436  ...   \n",
            "75%       0.962550     0.813090     0.970790     0.257875     0.215170  ...   \n",
            "max       2.274400     2.256800     2.219700     1.694600     1.177000  ...   \n",
            "\n",
            "               215          216          217          218          219  \\\n",
            "count  8743.000000  8743.000000  8743.000000  8743.000000  8743.000000   \n",
            "mean      0.039921     0.236045     0.017460    -0.000181     0.099152   \n",
            "std       0.363500     0.499744     0.334342     0.315676     0.422491   \n",
            "min      -1.135700    -1.425500    -1.293200    -1.204800    -1.247600   \n",
            "25%      -0.196505    -0.124815    -0.200560    -0.212570    -0.181110   \n",
            "50%       0.020447     0.142230     0.010184    -0.003301     0.058561   \n",
            "75%       0.248805     0.524955     0.224765     0.216025     0.315955   \n",
            "max       1.841900     2.091700     1.896100     1.327000     2.203600   \n",
            "\n",
            "               220          221          222          223          224  \n",
            "count  8743.000000  8743.000000  8743.000000  8743.000000  8743.000000  \n",
            "mean      0.589612     0.293870     0.505508     0.336356     3.002974  \n",
            "std       0.557340     0.526766     0.566306     0.539380     1.420828  \n",
            "min      -0.996050    -1.199500    -1.128500    -1.262700     1.000000  \n",
            "25%       0.150170    -0.092223     0.047072    -0.066114     2.000000  \n",
            "50%       0.632400     0.200250     0.491010     0.244160     3.000000  \n",
            "75%       1.034800     0.658455     0.975250     0.737990     4.000000  \n",
            "max       2.311800     2.306800     2.106100     2.272200     5.000000  \n",
            "\n",
            "[8 rows x 225 columns]\n",
            "\n",
            "Analyzing Feature Ranges...\n",
            "Range Ratio (Max/Min): 1.87\n",
            "Standard Deviation Ratio (Max/Min): 4.57\n",
            "\n",
            "Recommendation: No scaling or normalization required.\n",
            "Reason: Features are already on a similar scale.\n",
            "Testing data:\n",
            "Dataset Loaded Successfully!\n",
            "\n",
            "Basic Statistics:\n",
            "               0            1            2            3            4    \\\n",
            "count  6955.000000  6955.000000  6955.000000  6955.000000  6955.000000   \n",
            "mean      0.361653     0.009509     0.334088     0.170567     0.181238   \n",
            "std       0.547173     0.326070     0.540563     0.475084     0.466655   \n",
            "min      -1.037900    -1.130500    -1.109600    -1.175200    -1.289700   \n",
            "25%      -0.054326    -0.212200    -0.079548    -0.152325    -0.145115   \n",
            "50%       0.281510     0.007155     0.240300     0.098940     0.111040   \n",
            "75%       0.778830     0.225490     0.730760     0.414670     0.434665   \n",
            "max       2.091900     1.691900     2.151800     2.075300     2.224200   \n",
            "\n",
            "               5            6            7            8            9    ...  \\\n",
            "count  6955.000000  6955.000000  6955.000000  6955.000000  6955.000000  ...   \n",
            "mean      0.522672     0.378666     0.509466     0.038795    -0.006561  ...   \n",
            "std       0.562330     0.543341     0.559478     0.364803     0.311439  ...   \n",
            "min      -0.986980    -1.160500    -1.067300    -1.103200    -1.312500  ...   \n",
            "25%       0.071536    -0.031810     0.065200    -0.201020    -0.218580  ...   \n",
            "50%       0.508920     0.299280     0.506040     0.015704    -0.012001  ...   \n",
            "75%       0.976965     0.790505     0.954585     0.241665     0.204975  ...   \n",
            "max       2.249600     2.236400     2.150100     1.705400     1.152700  ...   \n",
            "\n",
            "               214          215          216          217          218  \\\n",
            "count  6955.000000  6955.000000  6955.000000  6955.000000  6955.000000   \n",
            "mean      0.007117     0.041603     0.246528     0.016712     0.003231   \n",
            "std       0.318003     0.362016     0.522797     0.338699     0.312707   \n",
            "min      -1.237000    -1.030700    -1.228300    -1.145200    -1.060500   \n",
            "25%      -0.197500    -0.193685    -0.123595    -0.207090    -0.206435   \n",
            "50%       0.004679     0.023074     0.150320     0.011380     0.003131   \n",
            "75%       0.215610     0.250870     0.546195     0.229320     0.213360   \n",
            "max       1.404800     1.928700     2.219400     1.671700     1.135700   \n",
            "\n",
            "               219          220          221          222          223  \n",
            "count  6955.000000  6955.000000  6955.000000  6955.000000  6955.000000  \n",
            "mean      0.097989     0.570747     0.281675     0.519232     0.313377  \n",
            "std       0.419728     0.553498     0.527142     0.563468     0.534210  \n",
            "min      -1.065000    -1.270900    -1.197500    -1.159100    -1.189000  \n",
            "25%      -0.176200     0.125885    -0.104325     0.069139    -0.074719  \n",
            "50%       0.050822     0.607450     0.179860     0.512020     0.219190  \n",
            "75%       0.306025     1.008150     0.620930     0.976380     0.688875  \n",
            "max       1.859900     2.078900     2.140700     2.339000     2.141000  \n",
            "\n",
            "[8 rows x 224 columns]\n",
            "\n",
            "Analyzing Feature Ranges...\n",
            "Range Ratio (Max/Min): 1.73\n",
            "Standard Deviation Ratio (Max/Min): 1.82\n",
            "\n",
            "Recommendation: No scaling or normalization required.\n",
            "Reason: Features are already on a similar scale.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ5lvEk1scZe"
      },
      "source": [
        "================================================================================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CaWHg5VscZf"
      },
      "source": [
        "Μοντέλα που δοκιμάστηκαν"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdUmY16dscZf"
      },
      "source": [
        "Συνάρτηση για fine tuning μέσω gridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5WZEjHyscZf"
      },
      "outputs": [],
      "source": [
        "# Ορισμός της συνάρτησης fine_tune_model\n",
        "def fine_tune_model(model, param_grid, X_train, y_train, cv=10, n_jobs=-1, verbose=2):\n",
        "    \"\"\"\n",
        "    Βελτιστοποιεί το δοθέν μοντέλο χρησιμοποιώντας GridSearchCV.\n",
        "\n",
        "    Παράμετροι:\n",
        "    - model: Το μοντέλο μηχανικής μάθησης που θα βελτιστοποιηθεί.\n",
        "    - param_grid: Το πλέγμα παραμέτρων για το GridSearchCV.\n",
        "    - X_train: Χαρακτηριστικά εκπαίδευσης.\n",
        "    - y_train: Ετικέτες εκπαίδευσης.\n",
        "    - cv: Αριθμός πτυχών διασταυρούμενης επικύρωσης (προεπιλογή είναι 3).\n",
        "    - n_jobs: Αριθμός εργασιών που θα εκτελεστούν παράλληλα (προεπιλογή είναι -1).\n",
        "    - verbose: Επίπεδο λεπτομέρειας (προεπιλογή είναι 2).\n",
        "\n",
        "    Επιστρέφει:\n",
        "    - best_model: Το καλύτερο μοντέλο που βρέθηκε από το GridSearchCV.\n",
        "    - best_params: Οι καλύτερες παράμετροι που βρέθηκαν από το GridSearchCV.\n",
        "    \"\"\"\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, n_jobs=n_jobs, verbose=verbose)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    # Εκτύπωση της ακρίβειας μετά από κάθε επανάληψη\n",
        "    means = grid_search.cv_results_['mean_test_score']\n",
        "    stds = grid_search.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
        "        print(f\"Mean accuracy: {mean:.3f} (+/-{std * 2:.3f}) for params: {params}\")\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "    return best_model, best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfYGi_4xscZg"
      },
      "source": [
        "Fine - Tune στο random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj2-BYxRscZg",
        "outputId": "364ffdb7-eb10-4395-c19d-54bec8f27490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8061749571183533\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      0.95      0.94       377\n",
            "           2       0.67      0.64      0.65       332\n",
            "           3       0.86      0.90      0.88       349\n",
            "           4       0.87      0.85      0.86       325\n",
            "           5       0.70      0.68      0.69       366\n",
            "\n",
            "    accuracy                           0.81      1749\n",
            "   macro avg       0.80      0.80      0.80      1749\n",
            "weighted avg       0.80      0.81      0.80      1749\n",
            "\n",
            "Best Validation Accuracy for Random Forest: 0.819325328759291\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      0.95      0.94       377\n",
            "           2       0.69      0.66      0.68       332\n",
            "           3       0.87      0.92      0.90       349\n",
            "           4       0.88      0.86      0.87       325\n",
            "           5       0.71      0.70      0.70       366\n",
            "\n",
            "    accuracy                           0.82      1749\n",
            "   macro avg       0.82      0.82      0.82      1749\n",
            "weighted avg       0.82      0.82      0.82      1749\n",
            "\n",
            "Best parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Validation Accuracy: 0.8061749571183533\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      0.95      0.94       377\n",
            "           2       0.67      0.64      0.65       332\n",
            "           3       0.86      0.90      0.88       349\n",
            "           4       0.87      0.85      0.86       325\n",
            "           5       0.70      0.68      0.69       366\n",
            "\n",
            "    accuracy                           0.81      1749\n",
            "   macro avg       0.80      0.80      0.80      1749\n",
            "weighted avg       0.80      0.81      0.80      1749\n",
            "\n",
            "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n",
            "Mean accuracy: 0.735 (+/-0.035) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.802 (+/-0.022) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.807 (+/-0.031) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.813 (+/-0.027) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.742 (+/-0.036) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.801 (+/-0.021) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.808 (+/-0.020) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.816 (+/-0.023) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.743 (+/-0.030) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.029) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.809 (+/-0.029) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.811 (+/-0.035) for params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Mean accuracy: 0.744 (+/-0.028) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.799 (+/-0.030) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.806 (+/-0.031) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.812 (+/-0.031) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.744 (+/-0.028) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.799 (+/-0.030) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.806 (+/-0.031) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.812 (+/-0.031) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.746 (+/-0.038) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.807 (+/-0.037) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.814 (+/-0.026) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.815 (+/-0.025) for params: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.035) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.026) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.805 (+/-0.025) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.804 (+/-0.030) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.035) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.026) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.805 (+/-0.025) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.804 (+/-0.030) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.035) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.026) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.805 (+/-0.025) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.804 (+/-0.030) for params: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Mean accuracy: 0.747 (+/-0.037) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.029) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.804 (+/-0.025) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.808 (+/-0.029) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.748 (+/-0.034) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.804 (+/-0.021) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.805 (+/-0.021) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.750 (+/-0.037) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.794 (+/-0.025) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.803 (+/-0.024) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.806 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Mean accuracy: 0.755 (+/-0.041) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.798 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.798 (+/-0.025) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.807 (+/-0.033) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.755 (+/-0.041) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.798 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.798 (+/-0.025) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.807 (+/-0.033) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.744 (+/-0.029) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.798 (+/-0.031) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.800 (+/-0.023) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.803 (+/-0.029) for params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.037) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.794 (+/-0.029) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.801 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.801 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.037) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.794 (+/-0.029) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.801 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.801 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.037) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.794 (+/-0.029) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.801 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.801 (+/-0.027) for params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Mean accuracy: 0.735 (+/-0.041) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.800 (+/-0.026) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.807 (+/-0.034) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.812 (+/-0.030) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.741 (+/-0.033) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.801 (+/-0.021) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.807 (+/-0.022) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.815 (+/-0.024) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.742 (+/-0.036) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.029) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.810 (+/-0.025) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.812 (+/-0.034) for params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Mean accuracy: 0.744 (+/-0.028) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.799 (+/-0.030) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.806 (+/-0.031) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.812 (+/-0.031) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.744 (+/-0.028) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.799 (+/-0.030) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.806 (+/-0.031) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.812 (+/-0.031) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.746 (+/-0.038) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.807 (+/-0.038) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.814 (+/-0.027) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.815 (+/-0.025) for params: {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.035) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.026) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Mean accuracy: 0.805 (+/-0.025) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Mean accuracy: 0.804 (+/-0.030) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.035) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.026) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 50}\n",
            "Mean accuracy: 0.805 (+/-0.025) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 100}\n",
            "Mean accuracy: 0.804 (+/-0.030) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "Mean accuracy: 0.753 (+/-0.035) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 10}\n",
            "Mean accuracy: 0.797 (+/-0.026) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Mean accuracy: 0.805 (+/-0.025) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Mean accuracy: 0.804 (+/-0.030) for params: {'max_depth': 30, 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 300}\n",
            "Best parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 300}\n",
            "accuracy_best: 0.8101772441395083\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      0.95      0.94       377\n",
            "           2       0.67      0.65      0.66       332\n",
            "           3       0.87      0.92      0.89       349\n",
            "           4       0.86      0.86      0.86       325\n",
            "           5       0.70      0.67      0.68       366\n",
            "\n",
            "    accuracy                           0.81      1749\n",
            "   macro avg       0.81      0.81      0.81      1749\n",
            "weighted avg       0.81      0.81      0.81      1749\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 5 - Fine-Tune Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 250, 300, 350], # Number of trees in the forest\n",
        "    'max_depth': [None, 1, 2, 5, 7, 10, 20, 25, 30],  # Maximum depth of each tree\n",
        "    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4, 10]\n",
        "}\n",
        "\n",
        "# Initialize the RF model\n",
        "rf_notune = RandomForestClassifier(bootstrap=True,random_state=42)\n",
        "\n",
        "# Train the initial RF model\n",
        "rf_notune.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred = rf_notune.predict(X_val)\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation Accuracy (Initial): {accuracy}\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "# silhouette = silhouette_score(X_test, rf_notune.predict(X_test))\n",
        "# print(f\"Silhouette Score(initial): {silhouette}\")\n",
        "# db_index = davies_bouldin_score(X_test, rf_notune.predict(X_test))\n",
        "# print(f\"Davies-Bouldin Index(initial): {db_index}\")\n",
        "\n",
        "rf = RandomForestClassifier(bootstrap=True,random_state=42)\n",
        "# Fine-tune the RF model\n",
        "best_rf, best_params_rf = fine_tune_model(rf, param_grid_rf, X_train, y_train)\n",
        "print(f\"Best parameters for KNN: {best_params_rf}\")\n",
        "\n",
        "# Predict with the best model\n",
        "y_val_pred_best = best_rf.predict(X_val)\n",
        "accuracy_best = accuracy_score(y_val, y_val_pred_best)\n",
        "print(f\"Validation Accuracy (Tuned): {accuracy_best}\")\n",
        "print(classification_report(y_val, y_val_pred_best))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4RCk3vuscZh"
      },
      "source": [
        "Fine - Tune στο SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMRYsLZLscZh",
        "outputId": "fe712064-3bb3-4e12-9869-d28df0bb6330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
            "Mean accuracy: 0.782 (+/-0.006) for params: {'C': 0.1, 'gamma': 1, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.203 (+/-0.000) for params: {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.782 (+/-0.006) for params: {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.203 (+/-0.000) for params: {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.782 (+/-0.006) for params: {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.784 (+/-0.006) for params: {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.782 (+/-0.006) for params: {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.702 (+/-0.012) for params: {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.751 (+/-0.011) for params: {'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.203 (+/-0.000) for params: {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.751 (+/-0.011) for params: {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.783 (+/-0.012) for params: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.751 (+/-0.011) for params: {'C': 1, 'gamma': 0.01, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.840 (+/-0.005) for params: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.751 (+/-0.011) for params: {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.788 (+/-0.006) for params: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.735 (+/-0.017) for params: {'C': 10, 'gamma': 1, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.203 (+/-0.000) for params: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.735 (+/-0.017) for params: {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.789 (+/-0.010) for params: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.735 (+/-0.017) for params: {'C': 10, 'gamma': 0.01, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.850 (+/-0.004) for params: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.735 (+/-0.017) for params: {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.806 (+/-0.009) for params: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.732 (+/-0.011) for params: {'C': 100, 'gamma': 1, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.203 (+/-0.000) for params: {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.732 (+/-0.011) for params: {'C': 100, 'gamma': 0.1, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.789 (+/-0.010) for params: {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.732 (+/-0.011) for params: {'C': 100, 'gamma': 0.01, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.847 (+/-0.001) for params: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "Mean accuracy: 0.732 (+/-0.011) for params: {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "Mean accuracy: 0.802 (+/-0.008) for params: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Best parameters for SVM: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "Validation Accuracy with Best SVM Model: 0.8639222412807318\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.94      0.97      0.96       377\n",
            "           2       0.73      0.76      0.74       332\n",
            "           3       0.92      0.93      0.93       349\n",
            "           4       0.93      0.89      0.91       325\n",
            "           5       0.80      0.76      0.78       366\n",
            "\n",
            "    accuracy                           0.86      1749\n",
            "   macro avg       0.86      0.86      0.86      1749\n",
            "weighted avg       0.86      0.86      0.86      1749\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 6 - Fine-Tune Support Vector Machine\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "svm = SVC(random_state=42)\n",
        "best_svm, best_params_svm = fine_tune_model(svm, param_grid_svm, X_train, y_train)\n",
        "print(f\"Best parameters for SVM: {best_params_svm}\")\n",
        "\n",
        "# Πρόβλεψη και αξιολόγηση στο validation set με το βελτιστοποιημένο μοντέλο\n",
        "y_val_pred_best_svm = best_svm.predict(X_val)\n",
        "accuracy_best_svm = accuracy_score(y_val, y_val_pred_best_svm)\n",
        "print(f\"Validation Accuracy with Best SVM Model: {accuracy_best_svm}\")\n",
        "print(classification_report(y_val, y_val_pred_best_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0uC0ElZscZh"
      },
      "source": [
        "Fine - Tune στο AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdunIL8gscZh",
        "outputId": "79125592-9aa9-4981-cb8d-ad515c720469"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy for AdaBoost: 0.6380789022298456\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.89      0.89      0.89       377\n",
            "           2       0.50      0.54      0.52       332\n",
            "           3       0.73      0.68      0.70       349\n",
            "           4       0.52      0.79      0.63       325\n",
            "           5       0.54      0.30      0.38       366\n",
            "\n",
            "    accuracy                           0.64      1749\n",
            "   macro avg       0.64      0.64      0.62      1749\n",
            "weighted avg       0.64      0.64      0.63      1749\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy for AdaBoost: 0.6958261863922242\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.92      0.91       377\n",
            "           2       0.54      0.58      0.56       332\n",
            "           3       0.74      0.74      0.74       349\n",
            "           4       0.63      0.74      0.68       325\n",
            "           5       0.64      0.49      0.55       366\n",
            "\n",
            "    accuracy                           0.70      1749\n",
            "   macro avg       0.69      0.69      0.69      1749\n",
            "weighted avg       0.70      0.70      0.69      1749\n",
            "\n",
            "Best parameters for AdaBoost: {'learning_rate': 0.1, 'n_estimators': 300}\n"
          ]
        }
      ],
      "source": [
        "# Cell - Fine-Tune AdaBoost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "param_grid_ab = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "ab = AdaBoostClassifier(random_state=42)\n",
        "ab.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Evaluate the Model\n",
        "# Predict on validation set\n",
        "y_val_pred_ab = ab.predict(X_val)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy_ab = accuracy_score(y_val, y_val_pred_ab)\n",
        "print(f\"Validation Accuracy for AdaBoost: {accuracy_ab}\")\n",
        "print(classification_report(y_val, y_val_pred_ab))\n",
        "\n",
        "# Perform grid search\n",
        "grid_search_ab = GridSearchCV(estimator=ab, param_grid=param_grid_ab, cv=5, scoring='accuracy')\n",
        "grid_search_ab.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params_ab = grid_search_ab.best_params_\n",
        "best_ab = grid_search_ab.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_val_pred_best_ab = best_ab.predict(X_val)\n",
        "accuracy_best_ab = accuracy_score(y_val, y_val_pred_best_ab)\n",
        "print(f\"Best Validation Accuracy for AdaBoost: {accuracy_best_ab}\")\n",
        "print(classification_report(y_val, y_val_pred_best_ab))\n",
        "print(f\"Best parameters for AdaBoost: {best_params_ab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrcmvK86scZi"
      },
      "source": [
        "Περαιτέρω εξερεύνηση για το SVM στην περιοχή των καλύτερων παραμέτρων\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKpNwTSlscZi",
        "outputId": "923bc09a-9eec-476f-d8fb-fa6e1fb358e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy for SVM: 0.8713550600343053\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.97      0.96       377\n",
            "           2       0.74      0.76      0.75       332\n",
            "           3       0.92      0.95      0.93       349\n",
            "           4       0.94      0.90      0.92       325\n",
            "           5       0.80      0.77      0.79       366\n",
            "\n",
            "    accuracy                           0.87      1749\n",
            "   macro avg       0.87      0.87      0.87      1749\n",
            "weighted avg       0.87      0.87      0.87      1749\n",
            "\n",
            "Best parameters for SVM: {'C': 8, 'gamma': 0.03, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Define the extended parameter grid\n",
        "param_grid_svm = {\n",
        "    'C': [8, 10, 12, 15],\n",
        "    'gamma': [0.015, 0.02, 0.025, 0.03],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=3, scoring='accuracy')\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params_svm = grid_search_svm.best_params_\n",
        "best_svm = grid_search_svm.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_val_pred_best_svm = best_svm.predict(X_val)\n",
        "accuracy_best_svm = accuracy_score(y_val, y_val_pred_best_svm)\n",
        "print(f\"Best Validation Accuracy for SVM: {accuracy_best_svm}\")\n",
        "print(classification_report(y_val, y_val_pred_best_svm))\n",
        "print(f\"Best parameters for SVM: {best_params_svm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr1D0h3uscZi"
      },
      "source": [
        "Περαιτέρω εξερεύνηση για SVM στην περιοχή των καλύτερων παραμέτρων\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPnjLGkqscZi",
        "outputId": "93107817-84e0-4794-f8e2-518d6ee094b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy for SVM: 0.873642081189251\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.97      0.96       377\n",
            "           2       0.74      0.76      0.75       332\n",
            "           3       0.93      0.94      0.93       349\n",
            "           4       0.93      0.90      0.92       325\n",
            "           5       0.81      0.78      0.80       366\n",
            "\n",
            "    accuracy                           0.87      1749\n",
            "   macro avg       0.87      0.87      0.87      1749\n",
            "weighted avg       0.87      0.87      0.87      1749\n",
            "\n",
            "Best parameters for SVM: {'C': 9, 'gamma': 0.025, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Define the extended parameter grid\n",
        "param_grid_svm = {\n",
        "    'C': [9, 10, 11, 12],\n",
        "    'gamma': [0.018, 0.02, 0.022, 0.025],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=3, scoring='accuracy')\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params_svm = grid_search_svm.best_params_\n",
        "best_svm = grid_search_svm.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_val_pred_best_svm = best_svm.predict(X_val)\n",
        "accuracy_best_svm = accuracy_score(y_val, y_val_pred_best_svm)\n",
        "print(f\"Best Validation Accuracy for SVM: {accuracy_best_svm}\")\n",
        "print(classification_report(y_val, y_val_pred_best_svm))\n",
        "print(f\"Best parameters for SVM: {best_params_svm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch2IXePSscZj"
      },
      "source": [
        "Περαιτέρω εξερεύνηση για SVM στην περιοχή των καλύτερων παραμέτρων\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGdx-rO1scZj",
        "outputId": "ceb54d8b-7f6e-4ba3-efd6-0f2914a932ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy for SVM: 0.8713550600343053\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.97      0.96       377\n",
            "           2       0.74      0.76      0.75       332\n",
            "           3       0.92      0.95      0.93       349\n",
            "           4       0.94      0.90      0.92       325\n",
            "           5       0.80      0.77      0.79       366\n",
            "\n",
            "    accuracy                           0.87      1749\n",
            "   macro avg       0.87      0.87      0.87      1749\n",
            "weighted avg       0.87      0.87      0.87      1749\n",
            "\n",
            "Best parameters for SVM: {'C': 8.5, 'gamma': 0.03, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Define the extended parameter grid\n",
        "param_grid_svm = {\n",
        "    'C': [8.5, 9, 9.5, 10],\n",
        "    'gamma': [0.022, 0.025, 0.028, 0.03],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=3, scoring='accuracy')\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params_svm = grid_search_svm.best_params_\n",
        "best_svm = grid_search_svm.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_val_pred_best_svm = best_svm.predict(X_val)\n",
        "accuracy_best_svm = accuracy_score(y_val, y_val_pred_best_svm)\n",
        "print(f\"Best Validation Accuracy for SVM: {accuracy_best_svm}\")\n",
        "print(classification_report(y_val, y_val_pred_best_svm))\n",
        "print(f\"Best parameters for SVM: {best_params_svm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzV5pXcMscZj"
      },
      "source": [
        "Fine - Tune στο XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk6dzH3QscZj",
        "outputId": "2becb124-3285-4a88-eeea-b0657750d059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy for XGBoost: 0.8502001143510578\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96       377\n",
            "           1       0.73      0.71      0.72       332\n",
            "           2       0.92      0.92      0.92       349\n",
            "           3       0.89      0.89      0.89       325\n",
            "           4       0.75      0.77      0.76       366\n",
            "\n",
            "    accuracy                           0.85      1749\n",
            "   macro avg       0.85      0.85      0.85      1749\n",
            "weighted avg       0.85      0.85      0.85      1749\n",
            "\n",
            "Best parameters for XGBoost: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.7}\n"
          ]
        }
      ],
      "source": [
        "# Adjust the labels to start from 0\n",
        "y_train_adjusted = y_train - 1\n",
        "y_val_adjusted = y_val - 1\n",
        "\n",
        "# Cell - Fine-Tune XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=3, scoring='accuracy')\n",
        "grid_search_xgb.fit(X_train, y_train_adjusted)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params_xgb = grid_search_xgb.best_params_\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_val_pred_best_xgb = best_xgb.predict(X_val)\n",
        "accuracy_best_xgb = accuracy_score(y_val_adjusted, y_val_pred_best_xgb)\n",
        "print(f\"Best Validation Accuracy for XGBoost: {accuracy_best_xgb}\")\n",
        "print(classification_report(y_val_adjusted, y_val_pred_best_xgb))\n",
        "print(f\"Best parameters for XGBoost: {best_params_xgb}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkz_RyAHscZj"
      },
      "source": [
        "Περαιτέρω εξερεύνηση για το XGBoost στην περιοχή των καλύτερων παραμέτρων\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azS09Y2_scZk",
        "outputId": "74336ba7-1006-4318-a02a-70eee2b2891f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jimki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy for XGBoost: 0.8502001143510578\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96       377\n",
            "           1       0.73      0.71      0.72       332\n",
            "           2       0.92      0.92      0.92       349\n",
            "           3       0.89      0.89      0.89       325\n",
            "           4       0.75      0.77      0.76       366\n",
            "\n",
            "    accuracy                           0.85      1749\n",
            "   macro avg       0.85      0.85      0.85      1749\n",
            "weighted avg       0.85      0.85      0.85      1749\n",
            "\n",
            "Best parameters for XGBoost: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.7}\n"
          ]
        }
      ],
      "source": [
        "# Cell - Fine-Tune XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define the narrowed parameter grid\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [250, 300, 350],\n",
        "    'learning_rate': [0.05, 0.1, 0.15],\n",
        "    'max_depth': [6, 7, 8],\n",
        "    'subsample': [0.65, 0.7, 0.75],\n",
        "    'colsample_bytree': [0.65, 0.7, 0.75]\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=3, scoring='accuracy')\n",
        "grid_search_xgb.fit(X_train, y_train_adjusted)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params_xgb = grid_search_xgb.best_params_\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_val_pred_best_xgb = best_xgb.predict(X_val)\n",
        "accuracy_best_xgb = accuracy_score(y_val_adjusted, y_val_pred_best_xgb)\n",
        "print(f\"Best Validation Accuracy for XGBoost: {accuracy_best_xgb}\")\n",
        "print(classification_report(y_val_adjusted, y_val_pred_best_xgb))\n",
        "print(f\"Best parameters for XGBoost: {best_params_xgb}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tune using LightGBM"
      ],
      "metadata": {
        "id": "EZ5DV_9p67Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_lgbm = {\n",
        "    'num_leaves': [31, 50, 70, 100, 150, 300],         # Number of leaves in one tree\n",
        "    'max_depth': [-1, 10, 20],         # Maximum depth of the tree (-1 means no limit)\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2], # Learning rate (shrinkage)\n",
        "    'n_estimators': [100, 200, 300],   # Number of boosting iterations (trees)\n",
        "    'min_child_samples': [10, 20, 30], # Minimum number of samples per leaf\n",
        "    'subsample': [0.8, 1.0],           # Subsample ratio of training instances\n",
        " }\n",
        "\n",
        "# Initialize the LGBM model\n",
        "lgbm_notune = LGBMClassifier()\n",
        "\n",
        "# Train the initial LGBM model\n",
        "lgbm_notune.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred = lgbm_notune.predict(X_val)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation Accuracy (Initial): {accuracy}\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "# silhouette = silhouette_score(X_test, rf_notune.predict(X_test))\n",
        "# print(f\"Silhouette Score(initial): {silhouette}\")\n",
        "# db_index = davies_bouldin_score(X_test, rf_notune.predict(X_test))\n",
        "# print(f\"Davies-Bouldin Index(initial): {db_index}\")\n",
        "\n",
        "lgbm = LGBMClassifier()\n",
        "# Fine-tune the MLP model\n",
        "best_lgbm, best_params_lgbm = fine_tune_model(lgbm, param_grid_lgbm, X_train, y_train)\n",
        "print(f\"Best parameters for LGBM: {best_params_lgbm}\")\n",
        "\n",
        "# Predict with the best model\n",
        "y_val_pred_best = best_lgbm.predict(X_val)\n",
        "accuracy_best = accuracy_score(y_val, y_val_pred_best)\n",
        "print(f\"Validation Accuracy (Tuned): {accuracy_best}\")\n",
        "print(classification_report(y_val, y_val_pred_best))"
      ],
      "metadata": {
        "id": "XMHo9Fhd66qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tune using KNN"
      ],
      "metadata": {
        "id": "HzK6gaOg8V9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter grid for KNN\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [5, 7, 9, 11, 13, 15],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
        "}\n",
        "# Initialize the KNN model\n",
        "knn_notune = KNeighborsClassifier()\n",
        "\n",
        "# Train the initial KNN model\n",
        "knn_notune.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred = knn_notune.predict(X_val)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation Accuracy (Initial): {accuracy}\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "silhouette = silhouette_score(X_test, knn_notune.predict(X_test))\n",
        "print(f\"Silhouette Score(initial): {silhouette}\")\n",
        "db_index = davies_bouldin_score(X_test, knn_notune.predict(X_test))\n",
        "print(f\"Davies-Bouldin Index(initial): {db_index}\")\n",
        "\n",
        "knn= KNeighborsClassifier()\n",
        "# Fine-tune the KNN model\n",
        "best_knn, best_params_knn = fine_tune_model(knn, param_grid_knn, X_train, y_train)\n",
        "print(f\"Best parameters for KNN: {best_params_knn}\")\n",
        "\n",
        "# Predict with the best model\n",
        "y_val_pred_best = best_knn.predict(X_val)\n",
        "accuracy_best = accuracy_score(y_val, y_val_pred_best)\n",
        "print(f\"Validation Accuracy (Tuned): {accuracy_best}\")\n",
        "print(classification_report(y_val, y_val_pred_best))\n",
        "\n",
        "\n",
        "# Example: Evaluate clustering\n",
        "silhouette = silhouette_score(X_test, best_knn.predict(X_test))\n",
        "print(f\"Silhouette Score: {silhouette}\")\n",
        "db_index = davies_bouldin_score(X_test, best_knn.predict(X_test))\n",
        "print(f\"Davies-Bouldin Index: {db_index}\")\n"
      ],
      "metadata": {
        "id": "98fx-R3c8Vki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tune using MLP"
      ],
      "metadata": {
        "id": "RVspUWY99YQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "# Parameter grid for MLP\n",
        "param_grid_mlp =\n",
        "{   'hidden_layer_sizes': [(10), (100), (50,50), (100,50), (100,100), (50,50,50), (400, 40), (400,38), (400,42)],\n",
        "    'activation': ['tanh', 'relu'],                    # Activation functions\n",
        "    'solver': ['sgd', 'adam'],                         # Optimizers\n",
        "    'alpha': [0.0001, 0.001, 0.01],                    # Regularization parameter\n",
        "    'learning_rate': ['constant', 'adaptive'],         # Learning rate schedule\n",
        "    'learning_rate_init': [0.001, 0.01],               # Initial learning rate\n",
        "}\n",
        "\n",
        "# Initialize the mlp model\n",
        "mlp_notune = MLPClassifier(random_state=42)\n",
        "\n",
        "# Train the initial MLP model\n",
        "mlp_notune.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred = mlp_notune.predict(X_val)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation Accuracy (Initial): {accuracy}\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "# silhouette = silhouette_score(X_test, rf_notune.predict(X_test))\n",
        "# print(f\"Silhouette Score(initial): {silhouette}\")\n",
        "# db_index = davies_bouldin_score(X_test, rf_notune.predict(X_test))\n",
        "# print(f\"Davies-Bouldin Index(initial): {db_index}\")\n",
        "\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "# Fine-tune the MLP model\n",
        "best_mlp, best_params_mlp = fine_tune_model(mlp, param_grid_mlp, X_train, y_train)\n",
        "print(f\"Best parameters for MLP: {best_params_mlp}\")\n",
        "\n",
        "# Predict with the best model\n",
        "y_val_pred_best = best_mlp.predict(X_val)\n",
        "accuracy_best = accuracy_score(y_val, y_val_pred_best)\n",
        "print(f\"Validation Accuracy (Tuned): {accuracy_best}\")\n",
        "print(classification_report(y_val, y_val_pred_best))\n"
      ],
      "metadata": {
        "id": "X-cUNjiA9Z0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}